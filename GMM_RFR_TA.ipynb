{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f71c4a6-316c-4194-9618-b9beabd42ae6",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model (GMM) + Random Forest Regression (RFR) Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75241d-adb5-4b62-9036-460c75fe62bc",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a3489-d738-4172-a7b8-2a099a32822e",
   "metadata": {},
   "source": [
    "#### This GMM is fit based on the GLODAP 2022 Data. Cluster assignments for new data will need to be predicted using this GMM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311c3e9-2204-4d0c-bbab-57147bf7541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMAL CLUSTER ALG: 8 CLUSTERS,NO LON, DEPTH as an additional predictor\n",
    "#ta_ed refers to the dataframe that is being used\n",
    "gmm_feat = [\"Latitude\", \"SST\", \"SSS\", \"Bottom Depth\"]\n",
    "gmm = GaussianMixture(n_components = 8, covariance_type = \"full\", random_state = 42)\n",
    "gmm.fit(ta_ed[gmm_feat])\n",
    "\n",
    "components = gmm.predict(ta_ed[gmm_feat])\n",
    "prob = gmm.predict_proba(ta_ed[gmm_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3cb38-6ebe-4f44-ba00-a8d28da2ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a cluster column that has the assigned clusters for each data point\n",
    "ta_ed[\"Cluster\"] = components\n",
    "## GMM outputs a probability that each data point belongs to each cluster; this code saves that information as a dataframe\n",
    "prob_df = pd.DataFrame(prob, columns = [\"Cluster 1\", \"Cluster 2\", \"Cluster 3\", \"Cluster 4\", \"Cluster 5\", \"Cluster 6\", \"Cluster 7\", \"Cluster 8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3dc3c-6699-4007-b345-15770a3c248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating new dataframes for each cluster\n",
    "clust1 = ta_ed.loc[ta_ed[\"Cluster\"] ==0]\n",
    "clust2 = ta_ed.loc[ta_ed[\"Cluster\"] ==1]\n",
    "clust3 = ta_ed.loc[ta_ed[\"Cluster\"] ==2]\n",
    "clust4 = ta_ed.loc[ta_ed[\"Cluster\"] ==3]\n",
    "clust5 = ta_ed.loc[ta_ed[\"Cluster\"] ==4]\n",
    "clust6 = ta_ed.loc[ta_ed[\"Cluster\"] ==5]\n",
    "clust7 = ta_ed.loc[ta_ed[\"Cluster\"] ==6]\n",
    "clust8 = ta_ed.loc[ta_ed[\"Cluster\"] ==7]\n",
    "\n",
    "#Create a list of dataframes; this is the input for the RFR code\n",
    "cluster_lst = [clust1, clust2, clust3, clust4, clust5, clust6, clust7, clust8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186d0e6-1f71-4177-b12a-3509c0981453",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62485c6e-7438-45a7-abe1-01d0834a1b51",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8a339-35be-4b3b-919c-3be98e32722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code iterates through a list of clusters and thus does not need to be individually run on each cluster\n",
    "\n",
    "# The clusters (e.g. clust1) should be dataframes and the dataframes should be stored in a list\n",
    "cluster_lst = [clust1, clust2, clust3, clust4, clust5, clust6, clust7, clust8]\n",
    "\n",
    "#MAKING FEATURES: make features specific for each cluster\n",
    "##Making a dictionary where the keys represent clusters\n",
    "def make_features(c_lst):\n",
    "    ft_dict = {}\n",
    "    for idx, cluster in enumerate(c_lst):\n",
    "        X1 = cluster[\"S\"] #0\n",
    "        X2 = cluster[\"PT\"] #1\n",
    "        X3 = cluster[\"Nitrate\"] #2\n",
    "        X4 = cluster[\"AOU\"] #3\n",
    "        X5 = cluster[\"Silicate\"] #4\n",
    "        lon20= np.cos(np.deg2rad(cluster[\"Longitude\"] - 20))#5\n",
    "        lon110= np.cos(np.deg2rad(cluster[\"Longitude\"] - 110)) #6\n",
    "        depth = cluster[\"Depth\"] #7\n",
    "        order = cluster[\"order\"] #8\n",
    "        y = cluster[\"TA\"] #9\n",
    "        cruise = cluster[\"Cruise\"] #10\n",
    "        ft_dict[idx]=[X1, X2, X3, X4, X5, lon20, lon110, depth, order, y, cruise]\n",
    "    return ft_dict\n",
    "\n",
    "#TRAIN-TEST SPLIT: split the data for each cluster\n",
    "##Don't want to split cruises between the training and testing groups\n",
    "def split_data(ft_dict):\n",
    "    split_dict = {}\n",
    "    for idx in ft_dict.keys(): #this is going thru each cluster\n",
    "        features = ft_dict[idx]\n",
    "        X, y, cruise = features[0], features[8], features[9]\n",
    "        clustersp = GroupShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 40) \n",
    "        ind = list(clustersp.split(X, y, cruise))\n",
    "        train_ind, test_ind = ind[0][0], ind[0][1]\n",
    "        split_dict[idx] = [train_ind, test_ind]\n",
    "    return split_dict\n",
    "\n",
    "#RESHAPED DATA: Reshape the data so it is organized in a way that the random forest can take the data\n",
    "##Drop y, order, and cruise columns here\n",
    "def rearrange(splitted_data, ft_dict):\n",
    "    reshaped_data = {}\n",
    "    for idx in splitted_data.keys(): #going into each of the keys/clusters\n",
    "        train_index, test_index = splitted_data[idx]\n",
    "        feat_values = []\n",
    "        for i in range(len(ft_dict[idx])-3): ##added a \"-3\" to get rid of last three elements (which are the y, order, and cruise columns)\n",
    "            re_values = []\n",
    "            train = np.array(ft_dict[idx][i])[train_index]\n",
    "            train.reshape(-1,1)\n",
    "            test = np.array(ft_dict[idx][i])[test_index]\n",
    "            test.reshape(-1,1)\n",
    "            re_values.append(train)\n",
    "            re_values.append(test)\n",
    "            feat_values.append(re_values)\n",
    "        reshaped_data[idx] = feat_values\n",
    "    return reshaped_data\n",
    "\n",
    "#TA (Y) VALUES: Making a dictionary of all the y-values for the training and testing data\n",
    "##Each of the keys is for a cluster\n",
    "def y_capture(splitted_data, ft_dict):\n",
    "    y_dict = {}\n",
    "    for idx in splitted_data.keys(): #going into each of the keys \n",
    "        train_index, test_index = splitted_data[idx]\n",
    "        y_train = ft_dict[idx][10].iloc[train_index]\n",
    "        y_test = ft_dict[idx][10].iloc[test_index]\n",
    "        y_dict[idx] = (y_train, y_test)\n",
    "    return y_dict\n",
    "\n",
    "#ORDER: Making a dictionary of all the order for the training and testing data\n",
    "##Each of the keys is for a cluster\n",
    "def order_capture(splitted_data, ft_dict): #this is for the order col\n",
    "    order_dict = {}\n",
    "    for idx in splitted_data.keys(): #going into each of the keys \n",
    "        train_index, test_index = splitted_data[idx]\n",
    "        order_train = ft_dict[idx][8].iloc[train_index]\n",
    "        order_test = ft_dict[idx][8].iloc[test_index]\n",
    "        order_dict[idx] = (order_train, order_test)\n",
    "    return order_dict\n",
    "\n",
    "#REARRANGING THE DATA INTO THE ESPER EQUATION 1: making sure the data is in a format \n",
    "def make_clust_eq(reshaped_dict):\n",
    "    tpose_dt = {}\n",
    "    for idx in reshaped_dict.keys(): #going into each cluster\n",
    "        final_set = []\n",
    "        train_eq_clust = []\n",
    "        test_eq_clust = []\n",
    "        for k in range(len(reshaped_dict[idx])):\n",
    "            train_eq_clust.append(reshaped_dict[idx][k][0]) #this is where the problem is \n",
    "            train_clust_trans = np.transpose(train_eq_clust)\n",
    "            test_eq_clust.append(reshaped_dict[idx][k][1]) #this is where the problem is \n",
    "            test_clust_trans = np.transpose(test_eq_clust)\n",
    "        final_set.append(train_clust_trans)\n",
    "        final_set.append(test_clust_trans)\n",
    "        tpose_dt[idx] = final_set\n",
    "    return tpose_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313adc8b-9197-4a99-876a-ba5ada8db9d0",
   "metadata": {},
   "source": [
    "### Running the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabcd10-e324-46bf-8472-360a1c006536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE DICTIONARY: run the random forest model to get the training and testing RMSEs for each cluster \n",
    "def rfr(tpose_dt, y_dict):\n",
    "    train_rmse_lst = []\n",
    "    tst_rmse_lst = []\n",
    "    for idx in tpose_dt.keys():\n",
    "        rf = RandomForestRegressor()\n",
    "        rf.fit(tpose_dt[idx][0], y_dict[idx][0])\n",
    "        \n",
    "        train_pred = rf.predict(tpose_dt[idx][0])\n",
    "        train_rmse = root_mean_squared_error(y_dict[idx][0], train_pred)\n",
    "        train_rmse_lst.append(train_rmse)\n",
    "        \n",
    "        test_pred = rf.predict(tpose_dt[idx][1])\n",
    "        tst_rmse = root_mean_squared_error(y_dict[idx][1], test_pred)\n",
    "        tst_rmse_lst.append(tst_rmse)\n",
    "    return train_rmse_lst, tst_rmse_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074a662-51d2-4672-b2d9-8de769241423",
   "metadata": {},
   "source": [
    "### Improving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480831dd-4b29-495c-b214-b56c45a9b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETER TUNING\n",
    "def hyp_tr(tpose_dt, y_dict):\n",
    "    p_dist = {\"n_estimators\": randint(50, 500),\n",
    "          \"max_depth\": randint(1,20),\n",
    "          \"max_features\": randint(1,9)}\n",
    "    hyp_dict = {}\n",
    "    for idx in tpose_dt.keys(): \n",
    "        rf = RandomForestRegressor()\n",
    "        rs = RandomizedSearchCV(rf, \n",
    "                        param_distributions = p_dist, \n",
    "                        n_iter = 5,\n",
    "                        cv =5)\n",
    "        res = rs.fit(tpose_dt[idx][0], y_dict[idx][0])\n",
    "        hyp_dict[idx] = [list(res.best_params_.values())[0],list(res.best_params_.values())[1], list(res.best_params_.values())[2]]\n",
    "    return hyp_dict\n",
    "\n",
    "#MODEL BASED ON CHOSEN HYPERPARAMETERS\n",
    "def best_model(tpose_dt, y_dict, hyp_dict):\n",
    "    best_rmse = {}\n",
    "    best_dict = {}\n",
    "    for idx in tpose_dt.keys():\n",
    "        best_rf = RandomForestRegressor(max_depth=hyp_dict[idx][0], n_estimators=hyp_dict[idx][1], max_features = hyp_dict[idx][2])\n",
    "        best_rf.fit(tpose_dt[idx][0], y_dict[idx][0])\n",
    "        best_preds = best_rf.predict(tpose_dt[idx][1])\n",
    "        rmse = root_mean_squared_error(y_dict[idx][1], best_preds)\n",
    "        best_rmse[idx] = rmse\n",
    "        best_dict[idx] = best_preds\n",
    "    return best_rmse, best_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97870de-0486-4341-9d08-90ef4141cec5",
   "metadata": {},
   "source": [
    "### Outputting TA Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f7172-f232-4c76-a9a2-36c1479c0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTION DICTIONARY: output a dictionary of all the training and testing set predictions\n",
    "def preds(tpose_dt, y_dict):\n",
    "    train_preds = {}\n",
    "    test_preds = {}\n",
    "    for idx in tpose_dt.keys():\n",
    "        rf = RandomForestRegressor()\n",
    "        rf.fit(tpose_dt[idx][0], y_dict[idx][0])\n",
    "        train_pred = rf.predict(tpose_dt[idx][0])\n",
    "        train_preds[idx] = train_pred\n",
    "        test_pred = rf.predict(tpose_dt[idx][1])\n",
    "        test_preds[idx] = test_pred\n",
    "    return train_preds, test_preds\n",
    "\n",
    "#PREDICTION DATAFRAME: output a dataframe of the testing set predictions with the order #, cluster #, and actual TA value\n",
    "def comb_ords_preds(test_preds, order_dict, y_dict):\n",
    "    clnum = []\n",
    "    ord_tst = []\n",
    "    pred_tst = []\n",
    "    actual_y = []\n",
    "    for idx in range(len(orders)):\n",
    "        cl_len = [idx]*len(orders[idx][1])\n",
    "        ord_tst.extend(orders[idx][1])\n",
    "        clnum.extend(cl_len)\n",
    "    for idx in range(len(y_dict)):\n",
    "        actual_y.extend(y_dict[idx][1])\n",
    "    for i in range(len(test_preds)):\n",
    "        pred_tst.extend(test_preds[i])\n",
    "    comb_df = pd.DataFrame( {\"Order_Num\": ord_tst,\n",
    "                             \"RFR1\": pred_tst,\n",
    "                             \"Y_test\": actual_y,\n",
    "                             \"Cluster\": clnum})\n",
    "    return comb_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
